{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score,cross_validate,train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Desktop/learn-together/train.csv\" , index_col=['Id'])\n",
    "df_test = pd.read_csv(\"Desktop/learn-together/test.csv\" , index_col=['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:54], df_train.loc[:,'Cover_Type'], \n",
    "                                                    test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbc = LGBMClassifier(n_estimators=500, learning_rate= 0.1, \n",
    "               objective= 'multiclass', num_class=7,\n",
    "               random_state= 12345, n_jobs=-1)\n",
    "lgbc.fit(X_train, y_train)\n",
    "pred = lgbc.predict(X_test)\n",
    "print(classification_report(pred, y_test, labels=None))\n",
    "lgbc_feature_importances = pd.DataFrame(lgbc.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=[\"importance\"])\n",
    "print(lgbc_feature_importances.sort_values(\"importance\",ascending=False))\n",
    "print(X_train.columns[lgbc_feature_importances[\"importance\"] == 0])\n",
    "\n",
    "def get_LGBC():\n",
    "    return LGBMClassifier(n_estimators=500, learning_rate= 0.1, \n",
    "               objective= 'multiclass', num_class=7,\n",
    "               random_state= 12345, n_jobs=-1)\n",
    "\n",
    "\n",
    "for thre in [0,50,100,200,500]:\n",
    "    print(np.mean(cross_val_score(get_LGBC(), \n",
    "                                  X_train.drop(X_train.columns[lgbc.feature_importances_<thre], axis=1), \n",
    "                                  y_train, cv=5)))\n",
    "\n",
    "X_train.drop(X_train.columns[lgbc.feature_importances_ == 0], axis=1, inplace=True)\n",
    "X_test.drop(X_test.columns[lgbc.feature_importances_ == 0], axis=1, inplace=True)\n",
    "\n",
    "fr = DecisionTreeClassifier(random_state=12345).fit(X_train, y_train)\n",
    "print(fr.score(X_test, y_test))\n",
    "fr_feature_importances = pd.DataFrame(fr.feature_importances_, \n",
    "                            index = X_train.columns,\n",
    "                             columns=['importance'])\n",
    "print(fr_feature_importances.sort_values(\"importance\",ascending=False))\n",
    "print(X_train.columns[fr_feature_importances[\"importance\"] == 0])\n",
    "\n",
    "for thre in [0,0.0001,0.001,0.005,0.01]:\n",
    "    print(np.mean(cross_val_score(DecisionTreeClassifier(random_state=12345), \n",
    "                                    X_train.drop(X_train.columns[fr.feature_importances_<thre], axis=1), \n",
    "                                    y_train, cv=5)))\n",
    "\n",
    "X_train.drop(X_train.columns[fr.feature_importances_ == 0], axis=1, inplace=True)\n",
    "X_test.drop(X_test.columns[fr.feature_importances_ == 0], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "ab_clf = AdaBoostClassifier(n_estimators=200,\n",
    "                            base_estimator=DecisionTreeClassifier(\n",
    "                                min_samples_leaf=2,\n",
    "                                random_state=12345),\n",
    "                            random_state=12345)\n",
    "   \n",
    "rf_clf = RandomForestClassifier(n_estimators=300,\n",
    "                                random_state=12345,\n",
    "                                n_jobs=1)\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators = 500, \n",
    "                        booster='gbtree', \n",
    "                        colsample_bylevel=1, \n",
    "                        colsample_bynode=1, \n",
    "                        colsample_bytree=0.8, \n",
    "                        gamma=5,\n",
    "                        nthread=1, \n",
    "                        learning_rate=0.1,\n",
    "                        max_delta_step=0, \n",
    "                        max_depth=10,\n",
    "                        min_child_weight=10, \n",
    "                        missing=None, \n",
    "                        random_state= 12345,\n",
    "                        n_jobs=1)                     \n",
    "\n",
    "et_clf = ExtraTreesClassifier(n_estimators=300,\n",
    "                              min_samples_leaf=1,\n",
    "                              min_samples_split=2,\n",
    "                              max_depth=50,\n",
    "                              max_features=0.3,\n",
    "                              bootstrap = False,\n",
    "                              random_state=12345,\n",
    "                              n_jobs=1)\n",
    "\n",
    "lg_clf = LGBMClassifier(n_estimators=300,\n",
    "                        num_leaves=128,\n",
    "                        learning_rate= 0.1,\n",
    "                        verbose=-1,\n",
    "                        num_class=7,\n",
    "                        random_state=12345,\n",
    "                        n_jobs=1)\n",
    "\n",
    "ensemble = [(\"AdaBoostClassifier\", ab_clf),\n",
    "            (\"RandomForestClassifier\", rf_clf),\n",
    "            (\"XGBClassifier\", xgb_clf),\n",
    "            (\"ExtraTreesClassifier\", et_clf),\n",
    "            (\"LGBMClassifier\", lg_clf)]\n",
    "\n",
    "print('> Cross-validating classifiers')\n",
    "for label, clf in ensemble:\n",
    "    score = cross_val_score(clf, X_train, y_train,\n",
    "                            cv=5,\n",
    "                            scoring='accuracy',\n",
    "                            verbose=0,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "    print('  -- {: <24} : {:.3f} : {}'.format(label, np.mean(score), np.around(score, 3)))\n",
    "    \n",
    "\n",
    "print('> Fitting stack')\n",
    "\n",
    "stack = StackingCVClassifier(classifiers=[ab_clf, rf_clf, xgb_clf, et_clf, lg_clf],\n",
    "                             meta_classifier=rf_clf,\n",
    "                             cv=5,\n",
    "                             stratify=True,\n",
    "                             shuffle=True,\n",
    "                             use_probas=True,\n",
    "                             use_features_in_secondary=True,\n",
    "                             verbose=1,\n",
    "                             random_state=12345,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "stack = stack.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "print('> Making predictions')\n",
    "pred = stack.predict(X_test)\n",
    "print(classification_report(pred, y_test, labels=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
